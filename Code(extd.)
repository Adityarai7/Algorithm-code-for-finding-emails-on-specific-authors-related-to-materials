import requests
from bs4 import BeautifulSoup
import re
import time
import logging

# Set up logging for better debugging and tracking
logging.basicConfig(
    filename="email_scraper.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

def fetch_webpage(url, headers, retries=3):
    for attempt in range(retries):
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                return response.text
            else:
                logging.warning(f"Failed to fetch {url}, status code: {response.status_code}")
        except requests.exceptions.RequestException as e:
            logging.error(f"Error fetching {url}: {e}")
        time.sleep(2)  # Wait before retrying
    return None

def parse_search_results(html_content, max_results=10):
    
    soup = BeautifulSoup(html_content, 'html.parser')
    results = soup.find_all("div", class_="gs_ri", limit=max_results)
    links = []
    for result in results:
        link_tag = result.find("a")
        if link_tag and link_tag.get("href"):
            links.append(link_tag.get("href"))
    return links

def 
    emails = re.findall(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", html_content)
    return list(set(emails))  # Remove duplicates

def save_emails_to_file(emails, filename="emails_found.txt"):
    
    with open(filename, "w") as file:
        for email in emails:
            file.write(email + "\n")
    logging.info(f"Emails saved to {filename}")

def find_author_emails(query, max_results=10):
    
    search_url = f"https://scholar.google.com/scholar?q={query.replace(' ', '+')}"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }

    logging.info(f"Starting email search for query: {query}")
    search_page_content = fetch_webpage(search_url, headers)

    if not search_page_content:
        logging.error("Failed to fetch search results.")
        return []

    article_links = parse_search_results(search_page_content, max_results)
    logging.info(f"Found {len(article_links)} article links.")

    all_emails = []
    for idx, article_url in enumerate(article_links, start=1):
        logging.info(f"Fetching article {idx}/{len(article_links)}: {article_url}")
        article_content = fetch_webpage(article_url, headers)
        if article_content:
            emails = extract_emails_from_page(article_content)
            logging.info(f"Found {len(emails)} emails on page.")
            all_emails.extend(emails)

    unique_emails = list(set(all_emails))
    logging.info(f"Total unique emails found: {len(unique_emails)}")

    if unique_emails:
        save_emails_to_file(unique_emails)
    return unique_emails


# Example Usage
if __name__ == "__main__":
    query = "materials science author John Doe"
    emails = find_author_emails(query, max_results=5)
    
    if emails:
        print("Emails found:")
        for email in emails:
            print(email)
    else:
        print("No emails found.")
